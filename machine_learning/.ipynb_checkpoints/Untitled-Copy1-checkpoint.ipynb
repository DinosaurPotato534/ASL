{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ca551f2-7f62-44d0-a8ac-efeb76b197e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91936a48-decb-4079-98c3-51e44475535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_training = \"/Users/aaryangulia/Downloads/archive/asl_alphabet_train/asl_alphabet_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61dc1677-c765-4740-8781-23fcd3df0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes\n",
    "num_classes = len(os.listdir(data_dir_training)) - 1\n",
    "\n",
    "# Define the image size and batch size\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Use ImageDataGenerator for data loading and augmentation\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale pixel values to [0, 1]\n",
    "    validation_split=0.2,  # Split data into training and validation sets\n",
    "    rotation_range=20,  # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20% of the width\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of the height\n",
    "    shear_range=0.2,  # Shear intensity (shear angle in radians)\n",
    "    zoom_range=0.2,  # Randomly zoom images by up to 20%\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    fill_mode='nearest'  # Strategy for filling in newly created pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba5adbf7-279a-47fa-b3d3-d96832c35920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69600 images belonging to 29 classes.\n",
      "Found 17400 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir_training,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # For multi-class classification\n",
    "    subset='training'  # Specify this as training data\n",
    ")\n",
    "\n",
    "# Create validation data generator\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_dir_training,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Specify this as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8dac4f7-8b59-44d6-adef-b78ec57254f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained MobileNetV2 model without the top classification layer\n",
    "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification layers for your specific task\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # num_classes is the number of classes in your dataset\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Now, you can train the model with your own dataset using model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dce2a4-cac6-40a4-a72d-a0bcf76c5f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model summary: \n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 1280)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 29)                37149     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2295133 (8.76 MB)\n",
      "Trainable params: 37149 (145.11 KB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      " Model Training: \n",
      "Epoch 1/5\n",
      "2175/2175 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.8296\n",
      "Epoch 1: saving model to model_weights.h5\n",
      "2175/2175 [==============================] - 1653s 759ms/step - loss: 0.6419 - accuracy: 0.8296 - val_loss: 0.9541 - val_accuracy: 0.7264\n",
      "Epoch 2/5\n",
      "1177/2175 [===============>..............] - ETA: 10:18 - loss: 0.3040 - accuracy: 0.9145"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(filepath='model_weights.h5', \n",
    "                                      save_weights_only=True, \n",
    "                                      save_best_only=False, \n",
    "                                      verbose=1)\n",
    "#training model\n",
    "print(\"\\n Model summary: \")\n",
    "print(model.summary())\n",
    "print(\"\\n Model Training: \")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,  # You can adjust the number of epochs based on your training needs\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5afeb-0848-4d2f-af0e-7a87a53ea00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
